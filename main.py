import os
import json
import glob
import re
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from pydantic import BaseModel

load_dotenv()

# === æ•°æ®æ¨¡å‹ ===
class Relationship(BaseModel):
    name: str
    relation: str
    status: str

class InventoryItem(BaseModel):
    item_name: str
    description: str

class Protagonist(BaseModel):
    name: str
    age: int
    level: str
    status: str
    personality: str
    abilities: List[str]
    goal: str

class ChapterState(BaseModel):
    chapter_index: int
    protagonist: Protagonist
    inventory: List[InventoryItem]
    relationships: List[Relationship]
    current_plot_summary: str

# === å…¨å±€å¤§æ¨¡å‹é…ç½®è·å–å™¨ ===
# ğŸš¨ é‡è¦æé†’ï¼šè¯·å‹¿ä¿®æ”¹ä»¥ä¸‹æ¨¡å‹é…ç½®ï¼Œè¿™äº›æ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„å›ºå®šé…ç½® ğŸš¨
class LLMConfigManager:
    @staticmethod
    def get_config(model_name: str) -> Dict[str, Any]:
        configs = {
            "deepseek_chat": {
                "provider": "openai",
                "model": "deepseek-chat",
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "base_url": "https://api.deepseek.com/v1",
                "temperature": 0.7
            },
            "deepseek_reasoner": {
                "provider": "openai",
                "model": "deepseek-reasoner", 
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "base_url": "https://api.deepseek.com/v1",
                "temperature": 0.7
            },
            "dsf5": {
                "provider": "openai",
                "model": "[ç¨³å®š]gemini-2.5-pro-preview-06-05-c",
                "api_key": os.getenv("DSF5_API_KEY"),
                "base_url": "https://api.sikong.shop/v1",
                "temperature": 0.7
            },
            "openai_gpt4": {
                "provider": "openai",
                "model": "gpt-4",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": None,
                "temperature": 0.7
            },
            "openai_gpt35": {
                "provider": "openai", 
                "model": "gpt-3.5-turbo",
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": None,
                "temperature": 0.7
            },
            "anthropic_claude": {
                "provider": "anthropic",
                "model": "claude-3-sonnet-20240229",
                "api_key": os.getenv("ANTHROPIC_API_KEY"),
                "base_url": None,
                "temperature": 0.7
            },
            "google_gemini": {
                "provider": "google",
                "model": "gemini-pro",
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "base_url": None,
                "temperature": 0.7
            }
        }
        # é»˜è®¤è¿”å›deepseek_chatæ¨¡å‹
        return configs.get(model_name, configs["deepseek_chat"])

# === å…¨å±€å¤§æ¨¡å‹è°ƒç”¨å™¨ ===
class LLMCaller:
    @staticmethod
    def call(
        messages: List[Dict[str, str]],
        model_name: str = "deepseek_chat",
        memory: Optional[Any] = None,
        temperature: Optional[float] = None
    ) -> str:
        config = LLMConfigManager.get_config(model_name)
        
        if temperature is not None:
            config["temperature"] = temperature
            
        # æ ¹æ®provideråˆ›å»ºå¯¹åº”çš„LLMå®ä¾‹
        if config["provider"] == "openai":
            from langchain_openai import ChatOpenAI
            llm_params = {
                "model": config["model"],
                "api_key": config["api_key"],
                "temperature": config["temperature"]
            }
            if config["base_url"]:
                llm_params["base_url"] = config["base_url"]
            llm = ChatOpenAI(**llm_params)
        elif config["provider"] == "anthropic":
            from langchain_anthropic import ChatAnthropic
            llm = ChatAnthropic(
                model=config["model"],
                api_key=config["api_key"],
                temperature=config["temperature"]
            )
        elif config["provider"] == "google":
            from langchain_google_genai import ChatGoogleGenerativeAI
            llm = ChatGoogleGenerativeAI(
                model=config["model"],
                google_api_key=config["api_key"],
                temperature=config["temperature"]
            )
        else:
            raise ValueError(f"Unsupported provider: {config['provider']}")
        
        # å¦‚æœæœ‰è®°å¿†ï¼Œä½¿ç”¨å¯¹è¯é“¾
        if memory:
            from langchain.chains import ConversationChain
            chain = ConversationChain(llm=llm, memory=memory, verbose=False)
            # å°†messagesè½¬æ¢ä¸ºå•ä¸ªè¾“å…¥
            user_input = messages[-1]["content"] if messages else ""
            return chain.predict(input=user_input)
        else:
            # ç›´æ¥è°ƒç”¨LLM
            from langchain_core.messages import HumanMessage, SystemMessage
            lang_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    lang_messages.append(SystemMessage(content=msg["content"]))
                else:
                    lang_messages.append(HumanMessage(content=msg["content"]))
            
            response = llm.invoke(lang_messages)
            return response.content

# === çŠ¶æ€ç®¡ç†å™¨ ===
class StateManager:
    def __init__(self, data_path: str = "./data"):
        self.data_path = data_path
        os.makedirs(self.data_path, exist_ok=True)

    def _find_latest_file(self, pattern: str, novel_id: Optional[str] = None) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶ï¼Œæ”¯æŒå°è¯´IDè¿‡æ»¤"""
        if novel_id:
            # å¦‚æœæŒ‡å®šäº†å°è¯´IDï¼Œæ·»åŠ IDå‰ç¼€åˆ°æ¨¡å¼ä¸­
            pattern = f"{novel_id}_{pattern}"
        
        files = glob.glob(os.path.join(self.data_path, pattern))
        if not files:
            return None
        
        def get_numeric_part(filename):
            # æå–æ–‡ä»¶åä¸­çš„ç« èŠ‚ç¼–å·ï¼ˆå¿½ç•¥å°è¯´IDéƒ¨åˆ†ï¼‰
            basename = os.path.basename(filename)
            if novel_id:
                # ç§»é™¤å°è¯´IDå‰ç¼€åå†æå–æ•°å­—
                basename = basename.replace(f"{novel_id}_", "", 1)
            numbers = re.findall(r'\d+', basename)
            return int(numbers[0]) if numbers else 0
        
        return max(files, key=get_numeric_part)

    def load_latest_state(self, novel_id: Optional[str] = None) -> Optional[ChapterState]:
        """åŠ è½½æœ€æ–°çŠ¶æ€ï¼Œæ”¯æŒå°è¯´IDè¿‡æ»¤"""
        latest_file = self._find_latest_file("chapter_*_state.json", novel_id)
        if not latest_file:
            return None

        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return ChapterState(**data)

    def save_state(self, state: ChapterState, novel_id: Optional[str] = None):
        """ä¿å­˜çŠ¶æ€ï¼Œæ”¯æŒå°è¯´ID"""
        if novel_id:
            file_path = os.path.join(
                self.data_path, 
                f"{novel_id}_chapter_{state.chapter_index:03d}_state.json"
            )
        else:
            # å…¼å®¹æ—§æ ¼å¼
            file_path = os.path.join(
                self.data_path, 
                f"chapter_{state.chapter_index:03d}_state.json"
            )
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(state.model_dump_json(indent=2))

    def load_world_bible(self, novel_id: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½ä¸–ç•Œè®¾å®šï¼Œæ”¯æŒå°è¯´IDè¿‡æ»¤"""
        latest_file = self._find_latest_file("world_bible_*.json", novel_id)
        if not latest_file:
            return {}

        with open(latest_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def save_world_bible(self, world_bible: Dict[str, Any], novel_id: Optional[str] = None, version: int = 0):
        """ä¿å­˜ä¸–ç•Œè®¾å®šï¼Œæ”¯æŒå°è¯´ID"""
        if novel_id:
            file_path = os.path.join(
                self.data_path,
                f"{novel_id}_world_bible_{version:02d}.json"
            )
        else:
            # å…¼å®¹æ—§æ ¼å¼
            file_path = os.path.join(
                self.data_path,
                f"world_bible_{version:02d}.json"
            )
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(world_bible, f, indent=2, ensure_ascii=False)
    
    def list_novel_states(self, novel_id: str) -> List[str]:
        """åˆ—å‡ºæŒ‡å®šå°è¯´çš„æ‰€æœ‰çŠ¶æ€æ–‡ä»¶"""
        pattern = f"{novel_id}_chapter_*_state.json"
        files = glob.glob(os.path.join(self.data_path, pattern))
        return sorted(files)
    
    def list_novels(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å°è¯´ID"""
        pattern = "*_chapter_*_state.json"
        files = glob.glob(os.path.join(self.data_path, pattern))
        novel_ids = set()
        
        for file_path in files:
            filename = os.path.basename(file_path)
            # æå–å°è¯´IDï¼ˆç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            parts = filename.split('_')
            if len(parts) >= 3 and parts[1] == 'chapter':
                novel_ids.add(parts[0])
        
        return sorted(list(novel_ids))

# === è®°å¿†åˆ†ç‰‡å­˜å‚¨ç®¡ç†å™¨ ===
class MemoryChunkManager:
    """åˆ†ç‰‡å­˜å‚¨ç®¡ç†å™¨ - å¤„ç†æ¶ˆæ¯çš„åˆ†ç‰‡å­˜å‚¨å’Œç´¢å¼•"""
    
    def __init__(self, chunk_size: int = 100):
        self.chunk_size = chunk_size
    
    def get_chunk_index(self, message_number: int) -> int:
        """è·å–æ¶ˆæ¯æ‰€å±çš„åˆ†ç‰‡ç´¢å¼•"""
        return (message_number - 1) // self.chunk_size + 1
    
    def get_chunk_range(self, chunk_index: int) -> tuple:
        """è·å–åˆ†ç‰‡çš„æ¶ˆæ¯èŒƒå›´ (start, end)"""
        start = (chunk_index - 1) * self.chunk_size + 1
        end = chunk_index * self.chunk_size
        return start, end
    
    def get_chunk_filename(self, session_id: str, chunk_index: int) -> str:
        """ç”Ÿæˆåˆ†ç‰‡æ–‡ä»¶å"""
        return f"{session_id}_chunk_{chunk_index:03d}.json"
    
    def calculate_required_chunks(self, start_msg: int, end_msg: int) -> List[int]:
        """è®¡ç®—éœ€è¦è¯»å–çš„åˆ†ç‰‡ç´¢å¼•åˆ—è¡¨"""
        start_chunk = self.get_chunk_index(start_msg)
        end_chunk = self.get_chunk_index(end_msg)
        return list(range(start_chunk, end_chunk + 1))

class MemoryCompressor:
    """è®°å¿†å‹ç¼©å™¨ - ç‹¬ç«‹çš„å‹ç¼©æ¨¡å—"""
    
    def __init__(self):
        pass
    
    def compress_messages(
        self, 
        messages: List[Dict[str, Any]], 
        model_name: str = "deepseek_chat",
        compression_prompt: str = ""
    ) -> str:
        """å‹ç¼©æ¶ˆæ¯åˆ—è¡¨ä¸ºæ‘˜è¦æ–‡æœ¬"""
        if not messages:
            return ""
        
        # æ„å»ºå‹ç¼©æç¤ºè¯
        if not compression_prompt:
            compression_prompt = """è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼š

å¯¹è¯å†å²ï¼š
{history}

è¯·è¿”å›å‹ç¼©åçš„æ‘˜è¦ï¼š"""
        
        # æ ¼å¼åŒ–å†å²è®°å½•
        history_text = self._format_messages_for_compression(messages)
        
        # è°ƒç”¨LLMè¿›è¡Œå‹ç¼©
        compress_messages = [
            {"role": "user", "content": compression_prompt.format(history=history_text)}
        ]
        
        try:
            compressed_summary = LLMCaller.call(compress_messages, model_name)
            return compressed_summary
        except Exception as e:
            print(f"å‹ç¼©å¤±è´¥: {e}")
            return self._fallback_compression(messages)
    
    def _format_messages_for_compression(self, messages: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯ç”¨äºå‹ç¼©"""
        formatted = []
        for i, msg in enumerate(messages, 1):
            role = msg.get('role', 'unknown')
            content = msg.get('content', '')
            formatted.append(f"{i}. [{role}]: {content}")
        return "\n".join(formatted)
    
    def _fallback_compression(self, messages: List[Dict[str, Any]]) -> str:
        """å‹ç¼©å¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆ"""
        if not messages:
            return ""
        
        # ç®€å•çš„æˆªå–å‹ç¼©
        total_chars = sum(len(msg.get('content', '')) for msg in messages)
        summary = f"åŒ…å«{len(messages)}æ¡æ¶ˆæ¯ï¼Œæ€»è®¡çº¦{total_chars}å­—ç¬¦çš„å¯¹è¯è®°å½•ã€‚"
        
        # æ·»åŠ æœ€åå‡ æ¡æ¶ˆæ¯çš„ç®€è¦ä¿¡æ¯
        if len(messages) > 0:
            last_msg = messages[-1]
            summary += f" æœ€åæ¶ˆæ¯: [{last_msg.get('role', 'unknown')}] {last_msg.get('content', '')[:50]}..."
        
        return summary

class MemoryIndexManager:
    """è®°å¿†ç´¢å¼•ç®¡ç†å™¨ - å¤„ç†ä¼šè¯ç´¢å¼•å’Œå…ƒæ•°æ®"""
    
    def __init__(self, memory_path: str):
        self.memory_path = memory_path
        self.chunks_path = os.path.join(memory_path, "chunks")
        self.summaries_path = os.path.join(memory_path, "summaries")
        os.makedirs(self.chunks_path, exist_ok=True)
        os.makedirs(self.summaries_path, exist_ok=True)
    
    def load_session_index(self, session_id: str) -> Dict[str, Any]:
        """åŠ è½½ä¼šè¯ç´¢å¼•"""
        index_file = os.path.join(self.memory_path, f"{session_id}_index.json")
        if os.path.exists(index_file):
            with open(index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # åˆ›å»ºæ–°ç´¢å¼•
            return {
                "session_id": session_id,
                "total_messages": 0,
                "chunks": {},  # {chunk_index: {"start": 1, "end": 100, "count": 100}}
                "summaries": {},  # {chunk_index: {"file": "summary_001.json", "created_at": "..."}}
                "created_at": time.time(),
                "last_updated": time.time()
            }
    
    def save_session_index(self, session_id: str, index_data: Dict[str, Any]):
        """ä¿å­˜ä¼šè¯ç´¢å¼•"""
        index_data["last_updated"] = time.time()
        index_file = os.path.join(self.memory_path, f"{session_id}_index.json")
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
    
    def update_chunk_info(self, session_id: str, chunk_index: int, start: int, end: int, count: int):
        """æ›´æ–°åˆ†ç‰‡ä¿¡æ¯"""
        index_data = self.load_session_index(session_id)
        index_data["chunks"][str(chunk_index)] = {
            "start": start,
            "end": end, 
            "count": count,
            "updated_at": time.time()
        }
        index_data["total_messages"] = max(index_data["total_messages"], end)
        self.save_session_index(session_id, index_data)
    
    def update_summary_info(self, session_id: str, chunk_index: int, summary_file: str):
        """æ›´æ–°æ‘˜è¦ä¿¡æ¯"""
        index_data = self.load_session_index(session_id)
        index_data["summaries"][str(chunk_index)] = {
            "file": summary_file,
            "created_at": time.time()
        }
        self.save_session_index(session_id, index_data)
    
    def get_chunk_info(self, session_id: str, chunk_index: int) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†ç‰‡ä¿¡æ¯"""
        index_data = self.load_session_index(session_id)
        return index_data["chunks"].get(str(chunk_index))
    
    def list_available_chunks(self, session_id: str) -> List[int]:
        """åˆ—å‡ºå¯ç”¨çš„åˆ†ç‰‡ç´¢å¼•"""
        index_data = self.load_session_index(session_id)
        return [int(k) for k in index_data["chunks"].keys()]

class MemoryManager:
    """å¢å¼ºçš„è®°å¿†ç®¡ç†å™¨ - æ”¯æŒåˆ†ç‰‡å­˜å‚¨ã€ç´¢å¼•å’Œå‹ç¼©"""
    
    def __init__(self, memory_path: str = "./memory", chunk_size: int = 100):
        self.memory_path = memory_path
        self.chunk_size = chunk_size
        os.makedirs(self.memory_path, exist_ok=True)

        # åˆå§‹åŒ–å­æ¨¡å—
        self.chunk_manager = MemoryChunkManager(chunk_size)
        self.compressor = MemoryCompressor()
        self.index_manager = MemoryIndexManager(memory_path)
    
    def save_message(self, session_id: str, message: Dict[str, Any]) -> int:
        """ä¿å­˜å•æ¡æ¶ˆæ¯ï¼Œè¿”å›æ¶ˆæ¯ç¼–å·"""
        # åŠ è½½ä¼šè¯ç´¢å¼•
        index_data = self.index_manager.load_session_index(session_id)
        
        # è®¡ç®—æ–°æ¶ˆæ¯ç¼–å·
        message_number = index_data["total_messages"] + 1
        chunk_index = self.chunk_manager.get_chunk_index(message_number)
        
        # åŠ è½½æˆ–åˆ›å»ºåˆ†ç‰‡
        chunk_file = os.path.join(
            self.index_manager.chunks_path,
            self.chunk_manager.get_chunk_filename(session_id, chunk_index)
        )
        
        if os.path.exists(chunk_file):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
        else:
            chunk_data = {"messages": []}
        
        # æ·»åŠ æ¶ˆæ¯
        message_with_meta = {
            "number": message_number,
            "timestamp": time.time(),
            **message
        }
        chunk_data["messages"].append(message_with_meta)
        
        # ä¿å­˜åˆ†ç‰‡
        with open(chunk_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, indent=2, ensure_ascii=False)
        
        # æ›´æ–°ç´¢å¼•
        start, end = self.chunk_manager.get_chunk_range(chunk_index)
        actual_end = min(end, message_number)
        self.index_manager.update_chunk_info(
            session_id, chunk_index, start, actual_end, len(chunk_data["messages"])
        )
        
        return message_number
    
    def load_messages_by_range(
        self,
        session_id: str,
        start_msg: int = 1,
        end_msg: Optional[int] = None,
        use_compression: bool = False,
        compression_model: str = "deepseek_chat",
        read_compressed: bool = False
    ) -> List[Dict[str, Any]]:
        """æŒ‰èŒƒå›´åŠ è½½æ¶ˆæ¯
        
        Args:
            session_id: ä¼šè¯ID
            start_msg: å¼€å§‹æ¶ˆæ¯ç¼–å·
            end_msg: ç»“æŸæ¶ˆæ¯ç¼–å·
            use_compression: æ˜¯å¦å®æ—¶å‹ç¼©ï¼ˆè¯»å–æ—¶ä¸´æ—¶å‹ç¼©ï¼‰
            compression_model: å‹ç¼©ä½¿ç”¨çš„æ¨¡å‹
            read_compressed: æ˜¯å¦è¯»å–å·²å‹ç¼©çš„è®°å¿†ï¼ˆä»summariesè¯»å–ï¼‰
        """
        # å¦‚æœè¦è¯»å–å·²å‹ç¼©çš„è®°å¿†
        if read_compressed:
            return self._load_compressed_summaries(session_id, start_msg, end_msg)
        
        # è·å–ä¼šè¯æ€»æ¶ˆæ¯æ•°
        index_data = self.index_manager.load_session_index(session_id)
        total_messages = index_data["total_messages"]
        
        if total_messages == 0:
            return []
        
        # å¤„ç†end_msgå‚æ•°
        if end_msg is None:
            end_msg = total_messages
        end_msg = min(end_msg, total_messages)
        
        if start_msg > end_msg:
            return []
        
        # è®¡ç®—éœ€è¦çš„åˆ†ç‰‡
        required_chunks = self.chunk_manager.calculate_required_chunks(start_msg, end_msg)
        
        all_messages = []
        for chunk_index in required_chunks:
            chunk_messages = self._load_chunk_messages(session_id, chunk_index, start_msg, end_msg)
            all_messages.extend(chunk_messages)
        
        # å¯é€‰å®æ—¶å‹ç¼©
        if use_compression and all_messages:
            compressed_summary = self.compressor.compress_messages(
                all_messages, compression_model
            )
            return [{
                "role": "system",
                "content": f"[å®æ—¶å‹ç¼©æ‘˜è¦] {compressed_summary}",
                "is_compressed": True,
                "compression_type": "realtime",
                "original_count": len(all_messages)
            }]
        
        return all_messages

    def _load_compressed_summaries(
        self,
        session_id: str,
        start_msg: int = 1,
        end_msg: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """åŠ è½½å·²å‹ç¼©çš„è®°å¿†æ‘˜è¦"""
        index_data = self.index_manager.load_session_index(session_id)
        summaries = index_data.get("summaries", {})
        
        if not summaries:
            return []
        
        # è®¡ç®—éœ€è¦çš„åˆ†ç‰‡èŒƒå›´
        start_chunk = self.chunk_manager.get_chunk_index(start_msg)
        if end_msg:
            end_chunk = self.chunk_manager.get_chunk_index(end_msg)
        else:
            # è·å–æœ€å¤§çš„chunkç´¢å¼•
            available_chunks = [int(k) for k in summaries.keys()]
            end_chunk = max(available_chunks) if available_chunks else start_chunk
        
        compressed_messages = []
        
        for chunk_index in range(start_chunk, end_chunk + 1):
            if str(chunk_index) in summaries:
                summary_info = summaries[str(chunk_index)]
                summary_file = summary_info["file"]
                summary_path = os.path.join(self.index_manager.summaries_path, summary_file)
                
                if os.path.exists(summary_path):
                    try:
                        with open(summary_path, 'r', encoding='utf-8') as f:
                            summary_data = json.load(f)
                        
                        compressed_messages.append({
                            "role": "system",
                            "content": f"[å‹ç¼©è®°å¿†-åˆ†ç‰‡{chunk_index}] {summary_data['compressed_summary']}",
                            "is_compressed": True,
                            "compression_type": "stored",
                            "chunk_index": chunk_index,
                            "original_count": summary_data.get("original_count", 0),
                            "compression_model": summary_data.get("compression_model", "unknown")
                        })
                    except Exception as e:
                        print(f"åŠ è½½å‹ç¼©æ‘˜è¦å¤±è´¥: {e}")
        
        return compressed_messages

    def load_recent_messages(
        self,
        session_id: str,
        count: int = 20,
        use_compression: bool = False,
        compression_model: str = "deepseek_chat",
        read_compressed: bool = False
    ) -> List[Dict[str, Any]]:
        """åŠ è½½æœ€è¿‘çš„Næ¡æ¶ˆæ¯
        
        Args:
            session_id: ä¼šè¯ID
            count: æ¶ˆæ¯æ•°é‡
            use_compression: æ˜¯å¦å®æ—¶å‹ç¼©
            compression_model: å‹ç¼©æ¨¡å‹
            read_compressed: æ˜¯å¦è¯»å–å·²å‹ç¼©çš„è®°å¿†
        """
        index_data = self.index_manager.load_session_index(session_id)
        total_messages = index_data["total_messages"]
        
        if total_messages == 0:
            return []
        
        start_msg = max(1, total_messages - count + 1)
        return self.load_messages_by_range(
            session_id, start_msg, total_messages, use_compression, compression_model, read_compressed
        )
    
    def compress_chunk(
        self,
        session_id: str,
        chunk_index: int,
        model_name: str = "deepseek_chat",
        compression_prompt: str = ""
    ) -> bool:
        """å‹ç¼©æŒ‡å®šåˆ†ç‰‡"""
        try:
            # åŠ è½½åˆ†ç‰‡æ¶ˆæ¯
            chunk_messages = self._load_chunk_messages(session_id, chunk_index)
            if not chunk_messages:
                return False
            
            # æ‰§è¡Œå‹ç¼©
            compressed_summary = self.compressor.compress_messages(
                chunk_messages, model_name, compression_prompt
            )
            
            # ä¿å­˜å‹ç¼©ç»“æœ
            summary_file = f"{session_id}_summary_{chunk_index:03d}.json"
            summary_path = os.path.join(self.index_manager.summaries_path, summary_file)
            
            summary_data = {
                "chunk_index": chunk_index,
                "original_count": len(chunk_messages),
                "compressed_summary": compressed_summary,
                "compression_model": model_name,
                "created_at": time.time()
            }
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, indent=2, ensure_ascii=False)
            
            # æ›´æ–°ç´¢å¼•
            self.index_manager.update_summary_info(session_id, chunk_index, summary_file)
            
            return True
            
        except Exception as e:
            print(f"å‹ç¼©åˆ†ç‰‡å¤±è´¥: {e}")
            return False
    
    def batch_compress_chunks(
        self,
        session_id: str,
        chunk_indices: List[int],
        model_name: str = "deepseek_chat",
        compression_prompt: str = ""
    ) -> Dict[int, bool]:
        """æ‰¹é‡å‹ç¼©åˆ†ç‰‡"""
        results = {}
        for chunk_index in chunk_indices:
            results[chunk_index] = self.compress_chunk(
                session_id, chunk_index, model_name, compression_prompt
            )
        return results
    
    def _load_chunk_messages(
        self,
        session_id: str,
        chunk_index: int,
        start_filter: Optional[int] = None,
        end_filter: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """åŠ è½½åˆ†ç‰‡ä¸­çš„æ¶ˆæ¯"""
        chunk_file = os.path.join(
            self.index_manager.chunks_path,
            self.chunk_manager.get_chunk_filename(session_id, chunk_index)
        )
        
        if not os.path.exists(chunk_file):
            return []
        
        try:
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            
            messages = chunk_data.get("messages", [])
            
            # åº”ç”¨èŒƒå›´è¿‡æ»¤
            if start_filter is not None or end_filter is not None:
                filtered_messages = []
                for msg in messages:
                    msg_num = msg.get("number", 0)
                    if start_filter is not None and msg_num < start_filter:
                        continue
                    if end_filter is not None and msg_num > end_filter:
                        continue
                    filtered_messages.append(msg)
                return filtered_messages
            
            return messages
            
        except Exception as e:
            print(f"åŠ è½½åˆ†ç‰‡å¤±è´¥: {e}")
            return []
    
    def get_session_stats(self, session_id: str) -> Dict[str, Any]:
        """è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        index_data = self.index_manager.load_session_index(session_id)
        available_chunks = self.index_manager.list_available_chunks(session_id)

        return {
            "session_id": session_id,
            "total_messages": index_data["total_messages"],
            "total_chunks": len(available_chunks),
            "compressed_chunks": len(index_data["summaries"]),
            "chunk_size": self.chunk_size,
            "created_at": index_data["created_at"],
            "last_updated": index_data["last_updated"]
        }
    
    def list_sessions(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰ä¼šè¯"""
        index_files = glob.glob(os.path.join(self.memory_path, "*_index.json"))
        sessions = []
        for file_path in index_files:
            filename = os.path.basename(file_path)
            session_id = filename.replace("_index.json", "")
            sessions.append(session_id)
        return sessions

# === è®°å¿†ç®¡ç†å™¨ ===
# é‡å‘½åEnhancedMemoryManagerä¸ºMemoryManagerï¼Œç»Ÿä¸€è®°å¿†ç®¡ç†æ¥å£

# === å°è¯´ç”Ÿæˆå™¨ ===
class NovelGenerator:
    def __init__(self, chunk_size: int = 100):
        self.state_manager = StateManager()
        self.memory_manager = MemoryManager(chunk_size=chunk_size)

    def generate_chapter(
        self,
        chapter_outline: str,
        model_name: str = "deepseek_chat",
        system_prompt: str = "",
        use_memory: bool = False,
        session_id: str = "default",
        use_state: bool = True,
        use_world_bible: bool = True,
        update_state: bool = False,
        recent_count: int = 20,
        use_compression: bool = False,
        compression_model: str = "deepseek_chat",
        read_compressed: bool = False,
        novel_id: Optional[str] = None
    ) -> str:
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # åŠ è½½å†å²è®°å½•
        if use_memory and recent_count > 0:
            history_messages = self.memory_manager.load_recent_messages(
                session_id=session_id,
                count=recent_count,
                use_compression=use_compression,
                compression_model=compression_model,
                read_compressed=read_compressed
            )
            messages.extend(history_messages)
        
        # æ„å»ºç”¨æˆ·è¾“å…¥ - ä½¿ç”¨æ›´è‡ªç„¶çš„æç¤ºè¯è¡¨è¾¾
        user_content = f"è¯·æ ¹æ®ä¸‹é¢çš„ç« èŠ‚ç»†çº²è¿›è¡Œå°è¯´å†…å®¹åˆ›ä½œï¼š\n\n{chapter_outline}"
        
        if use_state:
            state = self.state_manager.load_latest_state(novel_id)
            if state:
                user_content += f"\n\nå½“å‰çŠ¶æ€ï¼š{state.model_dump_json(indent=2)}"
        
        if use_world_bible:
            world_bible = self.state_manager.load_world_bible(novel_id)
            if world_bible:
                user_content += f"\n\nä¸–ç•Œè®¾å®šï¼š{json.dumps(world_bible, ensure_ascii=False, indent=2)}"
        
        user_message = {"role": "user", "content": user_content}
        messages.append(user_message)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿†
        if use_memory:
            self.memory_manager.save_message(session_id, user_message)
        
        # è°ƒç”¨LLM
        response = LLMCaller.call(messages, model_name)
        
        # ä¿å­˜AIå›å¤åˆ°è®°å¿†
        if use_memory:
            ai_message = {"role": "assistant", "content": response}
            self.memory_manager.save_message(session_id, ai_message)
            
            # å¦‚æœå¯ç”¨å‹ç¼©ï¼Œè‡ªåŠ¨å‹ç¼©æœ€æ–°çš„åˆ†ç‰‡
            if use_compression:
                try:
                    # è·å–å½“å‰ä¼šè¯çš„ç»Ÿè®¡ä¿¡æ¯
                    stats = self.memory_manager.get_session_stats(session_id)
                    total_chunks = stats.get("total_chunks", 0)
                    
                    # å‹ç¼©æœ€æ–°çš„åˆ†ç‰‡ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœªå‹ç¼©ï¼‰
                    if total_chunks > 0:
                        index_data = self.memory_manager.index_manager.load_session_index(session_id)
                        compressed_chunks = len(index_data.get("summaries", {}))
                        
                        # å¦‚æœæœ‰æœªå‹ç¼©çš„åˆ†ç‰‡ï¼Œå‹ç¼©æœ€æ–°çš„ä¸€ä¸ª
                        if total_chunks > compressed_chunks:
                            latest_chunk = total_chunks
                            success = self.memory_manager.compress_chunk(
                                session_id=session_id,
                                chunk_index=latest_chunk,
                                model_name=compression_model
                            )
                            if success:
                                print(f"è‡ªåŠ¨å‹ç¼©åˆ†ç‰‡ {latest_chunk} æˆåŠŸ")
                            else:
                                print(f"è‡ªåŠ¨å‹ç¼©åˆ†ç‰‡ {latest_chunk} å¤±è´¥")
                except Exception as e:
                    print(f"è‡ªåŠ¨å‹ç¼©å¤±è´¥: {e}")
        
        # ä¿å­˜ç« èŠ‚å†…å®¹ - å°è¯•ä»ç»†çº²ä¸­æå–ç« èŠ‚ç´¢å¼•
        chapter_index = self._extract_chapter_index(chapter_outline)
        if chapter_index is not None:
            self._save_chapter(response, chapter_index, novel_id)
        
        # çŠ¶æ€æ›´æ–° - å¦‚æœå¯ç”¨çŠ¶æ€æ›´æ–°ä¸”ä½¿ç”¨äº†çŠ¶æ€
        if update_state and use_state:
            current_state = self.state_manager.load_latest_state(novel_id)
            if current_state:
                print(f"æ­£åœ¨æ›´æ–°çŠ¶æ€...")
                try:
                    # è¯»å–çŠ¶æ€æ›´æ–°è§„åˆ™
                    update_rules_file = os.path.join("./prompts", "update_state_rules.txt")
                    update_system_prompt = ""
                    if os.path.exists(update_rules_file):
                        with open(update_rules_file, 'r', encoding='utf-8') as f:
                            update_system_prompt = f.read().strip()
                    
                    # è°ƒç”¨çŠ¶æ€æ›´æ–°
                    new_state = self.update_state(
                        chapter_content=response,
                        current_state=current_state,
                        model_name=model_name,
                        novel_id=novel_id,
                        system_prompt=update_system_prompt
                    )
                    print(f"çŠ¶æ€æ›´æ–°å®Œæˆï¼Œæ–°çŠ¶æ€å·²ä¿å­˜")
                except Exception as e:
                    print(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
        
        return response

    def update_state(
        self,
        chapter_content: str,
        current_state: ChapterState,
        model_name: str = "deepseek_chat",
        novel_id: Optional[str] = None,
        system_prompt: str = """
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¯”è¾ƒä¸€ä¸ªæ—§çš„JSONçŠ¶æ€å’Œä¸€æ®µæ–°çš„å°è¯´ç« èŠ‚å†…å®¹ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªæ›´æ–°åçš„JSONå¯¹è±¡ã€‚
**è§„åˆ™:**
1.  **ä»¥æ—§JSONä¸ºåŸºç¡€**: å®Œå…¨åŸºäºæˆ‘æä¾›çš„æ—§JSONçŠ¶æ€è¿›è¡Œä¿®æ”¹ã€‚
2.  **ä»æ–°ç« èŠ‚æå–å˜åŒ–**: é˜…è¯»æ–°çš„å°è¯´ç« èŠ‚ï¼Œæ‰¾å‡ºæ‰€æœ‰å¯¼è‡´çŠ¶æ€å˜åŒ–çš„äº‹ä»¶ï¼Œä¾‹å¦‚ï¼šä¸»è§’ç­‰çº§ã€å±æ€§æå‡ï¼›è·å¾—æˆ–å¤±å»äº†æ–°ç‰©å“ï¼›å­¦ä¼šäº†æ–°æŠ€èƒ½æˆ–åŠŸæ³•ï¼›äººé™…å…³ç³»å‘ç”Ÿå˜åŒ–ï¼›è§£é”äº†æ–°çš„ä»»åŠ¡æˆ–ç›®æ ‡ã€‚
3.  **æ›´æ–°æ•°å€¼ä¸æè¿°**: ç²¾ç¡®åœ°æ›´æ–°JSONæ–‡ä»¶ä¸­çš„æ•°å€¼å’Œæè¿°æ–‡å­—ã€‚ä¾‹å¦‚ï¼Œ"level"å­—æ®µè¦æ ¹æ®å°è¯´å†…å®¹åˆç†æå‡ã€‚
4.  **æ·»åŠ æ–°æ¡ç›®**: å¦‚æœæœ‰æ–°ç‰©å“æˆ–æ–°äººç‰©å…³ç³»ï¼Œå°±åœ¨å¯¹åº”çš„æ•°ç»„ä¸­æ·»åŠ æ–°çš„å¯¹è±¡ã€‚
5.  **æ›´æ–°å‰§æƒ…æ€»ç»“**: ä¿®æ”¹ `current_plot_summary` å­—æ®µï¼Œç®€è¦æ¦‚æ‹¬æœ¬ç« å‘ç”Ÿçš„æ ¸å¿ƒäº‹ä»¶ã€‚
6.  **ä¸¥æ ¼éµå®ˆæ ¼å¼**: ä½ çš„è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ªä¸‹é¢æä¾›çš„JSONæ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–ä»£ç å—æ ‡è®°ã€‚
"""
    ) -> ChapterState:
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        user_content = f"""
---
### **æ—§çš„çŠ¶æ€JSON**ï¼š{current_state.model_dump_json(indent=2)}
---

### **æœ¬ç« å°è¯´å†…å®¹**ï¼š{chapter_content}

---
è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆæ›´æ–°åçš„JSONå¯¹è±¡ï¼š
"""
        messages.append({"role": "user", "content": user_content})
        
        response = LLMCaller.call(messages, model_name)
        
        try:
            # æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                state_data = json.loads(json_match.group())
                new_state = ChapterState(**state_data)
                self.state_manager.save_state(new_state, novel_id)
                return new_state
        except Exception as e:
            print(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
        
        return current_state

    def chat(
        self,
        user_input: str,
        model_name: str = "deepseek_chat",
        system_prompt: str = "",
        session_id: str = "default",
        use_memory: bool = True,
        recent_count: int = 20,
        use_compression: bool = False,
        compression_model: str = "deepseek_chat",
        save_conversation: bool = True
    ) -> str:
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # åŠ è½½å†å²è®°å½•
        if use_memory and recent_count > 0:
            history_messages = self.memory_manager.load_recent_messages(
                session_id=session_id,
                count=recent_count,
                use_compression=use_compression,
                compression_model=compression_model
            )
            messages.extend(history_messages)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        user_message = {"role": "user", "content": user_input}
        messages.append(user_message)
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        if save_conversation:
            self.memory_manager.save_message(session_id, user_message)
        
        # è°ƒç”¨LLM
        response = LLMCaller.call(messages, model_name)
        
        # ä¿å­˜AIå›å¤
        if save_conversation:
            ai_message = {"role": "assistant", "content": response}
            self.memory_manager.save_message(session_id, ai_message)
        
        return response


    
    def load_memory_by_range(
        self,
        session_id: str,
        start_msg: int = 1,
        end_msg: Optional[int] = None,
        use_compression: bool = False,
        compression_model: str = "deepseek_chat"
    ) -> List[Dict[str, Any]]:
        """æŒ‰èŒƒå›´åŠ è½½è®°å¿†"""
        return self.memory_manager.load_messages_by_range(
            session_id=session_id,
            start_msg=start_msg,
            end_msg=end_msg,
            use_compression=use_compression,
            compression_model=compression_model
        )
    
    def compress_memory_chunk(
        self,
        session_id: str,
        chunk_index: int,
        model_name: str = "deepseek_chat",
        compression_prompt: str = ""
    ) -> bool:
        """å‹ç¼©æŒ‡å®šçš„è®°å¿†åˆ†ç‰‡"""
        return self.memory_manager.compress_chunk(
            session_id=session_id,
            chunk_index=chunk_index,
            model_name=model_name,
            compression_prompt=compression_prompt
        )
    
    def batch_compress_memory(
        self,
        session_id: str,
        chunk_indices: List[int],
        model_name: str = "deepseek_chat",
        compression_prompt: str = ""
    ) -> Dict[int, bool]:
        """æ‰¹é‡å‹ç¼©è®°å¿†åˆ†ç‰‡"""
        return self.memory_manager.batch_compress_chunks(
            session_id=session_id,
            chunk_indices=chunk_indices,
            model_name=model_name,
            compression_prompt=compression_prompt
        )
    
    def get_memory_stats(self, session_id: str) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        return self.memory_manager.get_session_stats(session_id)

    def _extract_chapter_index(self, chapter_outline: str) -> Optional[int]:
        """ä»ç« èŠ‚ç»†çº²ä¸­æå–ç« èŠ‚ç´¢å¼•"""
        import re
        
        # å°è¯•åŒ¹é…å„ç§ç« èŠ‚ç´¢å¼•æ ¼å¼
        patterns = [
            r'ç¬¬(\d+)ç« ',  # ç¬¬1ç« ã€ç¬¬10ç« 
            r'chapter[_\s]*(\d+)',  # chapter_1, chapter 1
            r'ç« èŠ‚[_\s]*(\d+)',  # ç« èŠ‚_1, ç« èŠ‚ 1
            r'ã€ç¬¬(\d+)ç« ',  # ã€ç¬¬1ç« 
        ]
        
        for pattern in patterns:
            match = re.search(pattern, chapter_outline, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›None
        return None

    def generate_multiple_versions(
        self,
        chapter_outline: str,
        num_versions: int = 3,
        model_name: str = "deepseek_chat",
        system_prompt: str = "",
        novel_id: Optional[str] = None
    ) -> List[str]:
        """ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬çš„ç« èŠ‚"""
        versions = []
        
        for i in range(num_versions):
            print(f"æ­£åœ¨ç”Ÿæˆç¬¬ {i+1} ä¸ªç‰ˆæœ¬...")
            version = self.generate_chapter(
                chapter_outline=chapter_outline,
                model_name=model_name,
                system_prompt=system_prompt,
                use_memory=False,  # å¤šç‰ˆæœ¬ç”Ÿæˆæ—¶ä¸ä½¿ç”¨è®°å¿†
                novel_id=novel_id
            )
            versions.append(version)
        
        # ä¿å­˜æ‰€æœ‰ç‰ˆæœ¬
        chapter_index = self._extract_chapter_index(chapter_outline)
        if chapter_index is not None:
            self._save_versions(versions, chapter_index, novel_id)
        
        return versions

    def _save_chapter(self, content: str, chapter_index: int, novel_id: Optional[str] = None):
        os.makedirs("./xiaoshuo", exist_ok=True)
        if novel_id:
            file_path = f"./xiaoshuo/{novel_id}_chapter_{chapter_index:03d}.txt"
        else:
            # å…¼å®¹æ—§æ ¼å¼
            file_path = f"./xiaoshuo/chapter_{chapter_index:03d}.txt"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

    def _save_versions(self, versions: List[str], chapter_index: int, novel_id: Optional[str] = None):
        os.makedirs("./versions", exist_ok=True)
        if novel_id:
            file_path = f"./versions/{novel_id}_chapter_{chapter_index}_versions.json"
        else:
            # å…¼å®¹æ—§æ ¼å¼
            file_path = f"./versions/chapter_{chapter_index}_versions.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump({
                "novel_id": novel_id,
                "chapter_index": chapter_index,
                "versions": versions,
                "created_at": time.time()
            }, f, indent=2, ensure_ascii=False)

# === ç¤ºä¾‹ä½¿ç”¨ ===
if __name__ == "__main__":
    # æµ‹è¯•æ¶æ„åˆå§‹åŒ–
    print("=== å°è¯´ç”Ÿæˆç³»ç»Ÿ - é«˜åº¦æ¨¡å—åŒ–æ¶æ„ ===")
    
    # 1. æµ‹è¯•é…ç½®ç®¡ç†å™¨
    print("\n1. æµ‹è¯•é…ç½®ç®¡ç†å™¨:")
    config = LLMConfigManager.get_config("deepseek_chat")
    print(f"   DeepSeek Chaté…ç½®: {config['provider']}, {config['model']}")
    print(f"   Base URL: {config['base_url']}")
    config_default = LLMConfigManager.get_config("")  # æµ‹è¯•é»˜è®¤
    print(f"   é»˜è®¤æ¨¡å‹: {config_default['model']}")
    
    # 2. æµ‹è¯•ç»„ä»¶åˆå§‹åŒ–
    print("\n2. æµ‹è¯•ç»„ä»¶åˆå§‹åŒ–:")
    generator = NovelGenerator(chunk_size=100)
    print("   âœ“ NovelGenerator åˆå§‹åŒ–æˆåŠŸ")
    print("   âœ“ StateManager åˆå§‹åŒ–æˆåŠŸ")
    print("   âœ“ MemoryManager åˆå§‹åŒ–æˆåŠŸ (åˆ†ç‰‡å­˜å‚¨+å‹ç¼©)")
    
    # æµ‹è¯•ä¸åŒåˆ†ç‰‡å¤§å°
    small_chunk_generator = NovelGenerator(chunk_size=50)
    print("   âœ“ NovelGenerator (å°åˆ†ç‰‡) åˆå§‹åŒ–æˆåŠŸ")
    
    # 3. æµ‹è¯•ä¼šè¯ç®¡ç†
    print("\n3. æµ‹è¯•ä¼šè¯ç®¡ç†:")
    sessions = generator.memory_manager.list_sessions()
    print(f"   å½“å‰ä¼šè¯åˆ—è¡¨: {sessions}")
    
    # 4. æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    print("\n4. ä½¿ç”¨ç¤ºä¾‹:")
    print("   # ç”Ÿæˆç« èŠ‚ (å¸¦è®°å¿†)")
    print("   generator.generate_chapter(chapter_plan, use_memory=True, recent_count=20)")
    print("   # å¯¹è¯èŠå¤© (å¸¦å‹ç¼©)")
    print("   generator.chat('ç»§ç»­å†™ä½œ', use_compression=True, recent_count=15)")
    print("   # æŒ‰èŒƒå›´åŠ è½½è®°å¿†")
    print("   generator.load_memory_by_range('session1', 1, 50, use_compression=True)")
    print("   # å‹ç¼©è®°å¿†åˆ†ç‰‡")
    print("   generator.compress_memory_chunk('session1', 1)")
    print("   # æ‰¹é‡å‹ç¼©")
    print("   generator.batch_compress_memory('session1', [1,2,3])")
    print("   # è·å–ç»Ÿè®¡ä¿¡æ¯")
    print("   generator.get_memory_stats('session1')")
    print("   # ç›´æ¥è°ƒç”¨LLM")
    print("   LLMCaller.call(messages, model_name='dsf5')")
    
    print("\n=== æ¶æ„æµ‹è¯•å®Œæˆ ===")
    print("æ‰€æœ‰ç»„ä»¶å·²æˆåŠŸåˆå§‹åŒ–ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼")