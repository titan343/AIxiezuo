# å°è¯´ç”Ÿæˆç³»ç»Ÿ - é«˜åº¦æ¨¡å—åŒ–æ¶æ„

## é¡¹ç›®æ¦‚è¿°
åŸºäºLangChainçš„æ™ºèƒ½å°è¯´ç”Ÿæˆç³»ç»Ÿï¼Œé‡‡ç”¨é«˜åº¦æ¨¡å—åŒ–å’Œæ’ä»¶å¼æ¶æ„è®¾è®¡ã€‚ç³»ç»Ÿé€šè¿‡å…¨å±€é…ç½®ç®¡ç†å™¨å’Œç»Ÿä¸€è°ƒç”¨æ¥å£å®ç°å®Œå…¨çš„å‚æ•°åŒ–æ§åˆ¶ï¼Œæ— ç¡¬ç¼–ç æç¤ºè¯æˆ–å ä½ç¬¦ã€‚

## æ ¸å¿ƒæ¶æ„ç‰¹æ€§
- ğŸ”§ **å…¨å±€é…ç½®ç®¡ç†** - ç»Ÿä¸€çš„å¤§æ¨¡å‹é…ç½®è·å–
- ğŸš€ **å…¨å±€LLMè°ƒç”¨å™¨** - æ’ä»¶å¼å¤§æ¨¡å‹è°ƒç”¨æ¥å£
- âš™ï¸ **æ¥å£å‚æ•°åŒ–** - æ‰€æœ‰åŠŸèƒ½é€šè¿‡å‚æ•°æ§åˆ¶
- ğŸ§© **é«˜åº¦æ¨¡å—åŒ–** - ç»„ä»¶é—´å®Œå…¨è§£è€¦
- ğŸ’¾ **æ™ºèƒ½çŠ¶æ€ç®¡ç†** - è‡ªåŠ¨çŠ¶æ€è¿½è¸ªå’Œæ›´æ–°
- ğŸ§  **å¢å¼ºè®°å¿†ç®¡ç†** - åˆ†ç‰‡å­˜å‚¨ã€ç´¢å¼•ã€å‹ç¼©çš„è®°å¿†ç³»ç»Ÿ
- ğŸš« **é›¶ç¡¬ç¼–ç ** - æ–‡æ¡£ä¸­æ‰€æœ‰ä»£ç ç¤ºä¾‹çš„å­—ç¬¦ä¸²éƒ½æ˜¯å‚æ•°ç¤ºä¾‹ï¼Œéç¡¬ç¼–ç 

## æ¶æ„è®¾è®¡

### 1. å…¨å±€å¤§æ¨¡å‹é…ç½®è·å–å™¨ (LLMConfigManager)
```python
config = LLMConfigManager.get_config("openai_gpt4")
# è¿”å›: {"provider": "openai", "model": "gpt-4", "api_key": "...", ...}
```

### 2. å…¨å±€å¤§æ¨¡å‹è°ƒç”¨å™¨ (LLMCaller)
```python
response = LLMCaller.call(
    messages=[{"role": "user", "content": "å†™ä¸€æ®µå°è¯´"}],
    model_name="openai_gpt4",
    memory=memory_obj,  # å¯é€‰
    temperature=0.8     # å¯é€‰
)
```

### 3. ä¸šåŠ¡ç»„ä»¶
- **NovelGenerator** - å°è¯´ç”Ÿæˆ (é›†æˆæ™ºèƒ½è®°å¿†ç®¡ç†)
- **StateManager** - çŠ¶æ€ç®¡ç†
- **MemoryManager** - æ™ºèƒ½è®°å¿†ç®¡ç† (åˆ†ç‰‡å­˜å‚¨+ç´¢å¼•+å‹ç¼©)

## æ”¯æŒçš„å¤§æ¨¡å‹

### é…ç½®åˆ—è¡¨
- `deepseek_chat` - DeepSeek-V3-0324 (é»˜è®¤æ¨¡å‹)
- `deepseek_reasoner` - DeepSeek-R1-0528  
- `dsf5` - Gemini-2.5-Pro-Preview (ç¨³å®šç‰ˆ)
- `openai_gpt4` - OpenAI GPT-4
- `openai_gpt35` - OpenAI GPT-3.5-turbo
- `anthropic_claude` - Anthropic Claude-3-Sonnet
- `google_gemini` - Google Gemini Pro

### ğŸš¨ æ¨¡å‹è¯¦ç»†é…ç½® (ä¸¥ç¦ä¿®æ”¹æ­¤éƒ¨åˆ† - ç”¨æˆ·å›ºå®šé…ç½®) ğŸš¨
```python
# DeepSeekæ¨¡å‹é…ç½®
"deepseek_chat": {
    "provider": "openai",
    "model": "deepseek-chat",
    "api_key": os.getenv("DEEPSEEK_API_KEY"),
    "base_url": "https://api.deepseek.com/v1",
    "temperature": 0.7
}

"deepseek_reasoner": {
    "provider": "openai", 
    "model": "deepseek-reasoner",
    "api_key": os.getenv("DEEPSEEK_API_KEY"),
    "base_url": "https://api.deepseek.com/v1",
    "temperature": 0.7
}

# DSF5æ¨¡å‹é…ç½®
"dsf5": {
    "provider": "openai",
    "model": "[ç¨³å®š]gemini-2.5-pro-preview-06-05-c",
    "api_key": os.getenv("DSF5_API_KEY"),
    "base_url": "https://api.sikong.shop/v1", 
    "temperature": 0.7
}
```

### ç¯å¢ƒå˜é‡
```env
DEEPSEEK_API_KEY=your_deepseek_key
DSF5_API_KEY=your_dsf5_key
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_API_KEY=your_google_key
```

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬å°è¯´ç”Ÿæˆ
```python
from main import NovelGenerator

# åˆå§‹åŒ–ç”Ÿæˆå™¨ (é»˜è®¤åˆ†ç‰‡å¤§å°100)
generator = NovelGenerator(chunk_size=100)

# è¯»å–æ¨¡ç‰ˆæç¤ºè¯
writer_role = read_template("001_writer_role.txt")
writing_rules = read_template("001_writing_rules.txt")
system_prompt = writer_role + "\n\n" + writing_rules

# å®šä¹‰ç« èŠ‚è®¡åˆ’ (åŒ…å«ç« èŠ‚çº²è¦å’Œå‰§æƒ…è®¾å®š)
chapter_plan = {
    "chapter_index": 1,
    "title": "å¼€å§‹",
    "main_plot": "ä¸»è§’å¼€å§‹ä¿®ç‚¼ä¹‹è·¯",
    "chapter_outline": [
        "å‘ç°ä¿®ç‚¼å¤©èµ‹",
        "è·å¾—ç¬¬ä¸€æœ¬åŠŸæ³•", 
        "é‡åˆ°ç¥ç§˜å±é™©"
    ],
    "target_word_count": 2500,
    "mood": "ç´§å¼ åˆºæ¿€"
}

# ç”Ÿæˆç« èŠ‚ (å¸¦è®°å¿†ç®¡ç†)
content = generator.generate_chapter(
    chapter_plan=chapter_plan,
    model_name="deepseek_chat",
    system_prompt=system_prompt,      # ä¼ å…¥æ‹¼æ¥å¥½çš„æ¨¡ç‰ˆæç¤ºè¯
    use_memory=True,                  # å¯ç”¨å†å²è®°å½•
    recent_count=20,                  # åŠ è½½æœ€è¿‘20æ¡æ¶ˆæ¯
    use_compression=False,            # False=è¯»å–åŸå§‹æ¶ˆæ¯ï¼ŒTrue=è¯»å–å‹ç¼©æ‘˜è¦
    use_state=True,
    use_world_bible=True,
    use_previous_chapters=True,       # å¯ç”¨å‰é¢ç« èŠ‚å†…å®¹è¯»å–
    previous_chapters_count=2         # è¯»å–å‰é¢2ç« çš„å†…å®¹
)
```

### 2. æ™ºèƒ½äº¤äº’è°ƒç”¨ï¼ˆå‘½ä»¤è¡Œä½¿ç”¨ï¼‰
```python
# åŸºæœ¬äº¤äº’è°ƒç”¨ï¼ˆç”¨äºå‘½ä»¤è¡Œè„šæœ¬ï¼‰
response = generator.chat(
    user_input="è¯·ç»§ç»­å†™ä¸‹ä¸€æ®µ",
    model_name="deepseek_chat",
    system_prompt="ä½ æ˜¯å†™ä½œåŠ©æ‰‹",
    session_id="novel_project_1",
    use_memory=True,              # å¯ç”¨è®°å¿†
    recent_count=20,              # åŠ è½½æœ€è¿‘20æ¡æ¶ˆæ¯
    use_compression=False,        # æ˜¯å¦å‹ç¼©å†å²è®°å½•
    compression_model="deepseek_chat",  # å‹ç¼©ä½¿ç”¨çš„æ¨¡å‹
    save_conversation=True        # æ˜¯å¦ä¿å­˜äº¤äº’è®°å½•
)

# å¸¦å‹ç¼©çš„é•¿æœŸäº¤äº’
response = generator.chat(
    user_input="å›é¡¾ä¸€ä¸‹å‰é¢çš„å‰§æƒ…å‘å±•",
    session_id="long_project",
    recent_count=50,              # åŠ è½½æ›´å¤šå†å²
    use_compression=True,         # å¯ç”¨å‹ç¼©ä»¥èŠ‚çœtoken
    compression_model="deepseek_chat"
)
```

### 3. çŠ¶æ€æ›´æ–°
```python
# è¯»å–çŠ¶æ€æ›´æ–°æ¨¡ç‰ˆ
update_rules = read_template("001_update_state_rules.txt")

# æ›´æ–°ç« èŠ‚çŠ¶æ€
new_state = generator.update_state(
    chapter_content="ç”Ÿæˆçš„ç« èŠ‚å†…å®¹",
    current_state=current_state,
    model_name="openai_gpt35",
    system_prompt=update_rules  # ä¼ å…¥æ¨¡ç‰ˆæç¤ºè¯
)
```

### 4. è®°å¿†ç®¡ç†åŠŸèƒ½

#### æŒ‰èŒƒå›´åŠ è½½è®°å¿†
```python
# åŠ è½½æŒ‡å®šèŒƒå›´çš„æ¶ˆæ¯
messages = generator.load_memory_by_range(
    session_id="novel_project_1",
    start_msg=1,                  # èµ·å§‹æ¶ˆæ¯ç¼–å·
    end_msg=50,                   # ç»“æŸæ¶ˆæ¯ç¼–å·
    use_compression=True,         # æ˜¯å¦å‹ç¼©
    compression_model="deepseek_chat"
)
```

#### å‹ç¼©è®°å¿†åˆ†ç‰‡
```python
# å‹ç¼©å•ä¸ªåˆ†ç‰‡
success = generator.compress_memory_chunk(
    session_id="novel_project_1",
    chunk_index=1,                # åˆ†ç‰‡ç´¢å¼•
    model_name="deepseek_chat",   # å‹ç¼©æ¨¡å‹
    compression_prompt="è‡ªå®šä¹‰å‹ç¼©æç¤ºè¯"  # å¯é€‰
)

# æ‰¹é‡å‹ç¼©åˆ†ç‰‡
results = generator.batch_compress_memory(
    session_id="novel_project_1",
    chunk_indices=[1, 2, 3],      # è¦å‹ç¼©çš„åˆ†ç‰‡åˆ—è¡¨
    model_name="deepseek_chat"
)
```

#### è·å–è®°å¿†ç»Ÿè®¡
```python
stats = generator.get_memory_stats("novel_project_1")
# è¿”å›: {"total_messages": 150, "total_chunks": 2, "compressed_chunks": 1, ...}
```

### 5. ç›´æ¥è°ƒç”¨LLM
```python
from main import LLMCaller

# ç®€å•è°ƒç”¨
response = LLMCaller.call(
    messages=[
        {"role": "system", "content": "ä½ æ˜¯å°è¯´å®¶"},
        {"role": "user", "content": "å†™ä¸€æ®µå¯¹è¯"}
    ],
    model_name="google_gemini"
)

# å¸¦è®°å¿†è°ƒç”¨
response = LLMCaller.call(
    messages=[{"role": "user", "content": "ç»§ç»­æ•…äº‹"}],
    model_name="openai_gpt4",
    memory=memory_object
)
```

## å‚æ•°è¯´æ˜

**é‡è¦è¯´æ˜**ï¼šä»¥ä¸‹æ‰€æœ‰å‚æ•°ç¤ºä¾‹ä¸­çš„å­—ç¬¦ä¸²ï¼ˆå¦‚"ä½ æ˜¯å°è¯´å®¶"ã€"è¯·ç»§ç»­å†™ä½œ"ç­‰ï¼‰éƒ½æ˜¯**å‚æ•°ç¤ºä¾‹**ï¼Œä¸æ˜¯ç¡¬ç¼–ç ã€‚å®é™…ä½¿ç”¨æ—¶è¯·ä¼ å…¥ä½ éœ€è¦çš„å…·ä½“å†…å®¹ã€‚

### NovelGenerator() åˆå§‹åŒ–å‚æ•°
- `chunk_size` (int) - åˆ†ç‰‡å¤§å°ï¼ˆæ¶ˆæ¯æ•°é‡ï¼‰ï¼Œé»˜è®¤100

### generate_chapter() å‚æ•°è¯¦è§£
- `chapter_plan` (dict) - ç« èŠ‚è®¡åˆ’ï¼Œå¿…éœ€ã€‚åŒ…å«ç« èŠ‚çº²è¦ã€å‰§æƒ…è®¾å®šç­‰ç»“æ„åŒ–æ•°æ®
- `model_name` (str) - æ¨¡å‹åç§°ï¼Œé»˜è®¤"deepseek_chat"
- `system_prompt` (str) - **ç³»ç»Ÿæç¤ºè¯ï¼Œé»˜è®¤ç©ºã€‚è¿™æ˜¯ä¼ å…¥æ¨¡ç‰ˆæç¤ºè¯çš„å…¥å£**
  - è°ƒç”¨å‰éœ€è¯»å–æ¨¡ç‰ˆæ–‡ä»¶ï¼š`writer_role.txt + writing_rules.txt`
  - ç¤ºä¾‹ï¼š`system_prompt = read_template("001_writer_role.txt") + "\n\n" + read_template("001_writing_rules.txt")`
- `use_memory` (bool) - æ˜¯å¦åŠ è½½å†å²è®°å½•ï¼Œé»˜è®¤False
- `session_id` (str) - ä¼šè¯IDï¼Œé»˜è®¤"default"
- `use_state` (bool) - æ˜¯å¦åŠ è½½è§’è‰²çŠ¶æ€JSONï¼Œé»˜è®¤True
- `use_world_bible` (bool) - æ˜¯å¦åŠ è½½ä¸–ç•Œè®¾å®šJSONï¼Œé»˜è®¤True
- `recent_count` (int) - åŠ è½½æœ€è¿‘Næ¡æ¶ˆæ¯ï¼Œé»˜è®¤20
- `use_compression` (bool) - **å†å²è®°å½•å‹ç¼©æ§åˆ¶ï¼Œé»˜è®¤False**
  - `False`: ä» `chunks/{session_id}_chunk_xxx.json` è¯»å–åŸå§‹æ¶ˆæ¯
  - `True`: ä» `summaries/{session_id}_summary_xxx.json` è¯»å–å‹ç¼©æ‘˜è¦
- `compression_model` (str) - å‹ç¼©æ—¶ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤"deepseek_chat"
- `use_previous_chapters` (bool) - **æ˜¯å¦è¯»å–å‰é¢ç« èŠ‚å†…å®¹ï¼Œé»˜è®¤False**
  - ä»xiaoshuoç›®å½•è¯»å–å‰é¢å·²ä¿å­˜çš„ç« èŠ‚æ–‡ä»¶å†…å®¹
  - ç¡®ä¿ç”Ÿæˆå†…å®¹ä¸æœ€æ–°çš„ç« èŠ‚æ–‡ä»¶ä¿æŒä¸€è‡´ï¼Œè§£å†³è®°å¿†ä¸æ–‡ä»¶ä¸åŒæ­¥é—®é¢˜
- `previous_chapters_count` (int) - è¯»å–å‰é¢ç« èŠ‚çš„æ•°é‡ï¼Œé»˜è®¤1ï¼ˆèŒƒå›´1-10ï¼‰

### update_state() å‚æ•°è¯¦è§£
- `chapter_content` (str) - ç« èŠ‚å†…å®¹ï¼Œå¿…éœ€ã€‚ç”¨äºåˆ†æçŠ¶æ€å˜åŒ–çš„å°è¯´æ–‡æœ¬
- `current_state` (ChapterState) - å½“å‰çŠ¶æ€å¯¹è±¡ï¼Œå¿…éœ€
- `model_name` (str) - æ¨¡å‹åç§°ï¼Œé»˜è®¤"deepseek_chat"
- `system_prompt` (str) - **çŠ¶æ€æ›´æ–°æç¤ºè¯ï¼Œé»˜è®¤å·²å†…ç½®å®Œæ•´è§„åˆ™**
  - è°ƒç”¨å‰å¯è¯»å–æ¨¡ç‰ˆæ–‡ä»¶ï¼š`update_state_rules.txt`
  - ç¤ºä¾‹ï¼š`system_prompt = read_template("001_update_state_rules.txt")`
  - å¦‚æœä¸ä¼ å…¥ï¼Œä½¿ç”¨å†…ç½®çš„çŠ¶æ€æ›´æ–°è§„åˆ™

### chat() å‚æ•°è¯¦è§£
- `user_input` (str) - ç”¨æˆ·è¾“å…¥ï¼Œå¿…éœ€
- `model_name` (str) - æ¨¡å‹åç§°ï¼Œé»˜è®¤"deepseek_chat"
- `system_prompt` (str) - ç³»ç»Ÿæç¤ºè¯ï¼Œé»˜è®¤ç©º
- `session_id` (str) - ä¼šè¯IDï¼Œé»˜è®¤"default"
- `use_memory` (bool) - æ˜¯å¦åŠ è½½å†å²è®°å½•ï¼Œé»˜è®¤True
- `recent_count` (int) - åŠ è½½æœ€è¿‘Næ¡æ¶ˆæ¯ï¼Œé»˜è®¤20
- `use_compression` (bool) - **å†å²è®°å½•å‹ç¼©æ§åˆ¶ï¼Œé»˜è®¤False**
  - `False`: ä»åŸå§‹æ¶ˆæ¯æ–‡ä»¶è¯»å–
  - `True`: ä»å‹ç¼©æ‘˜è¦æ–‡ä»¶è¯»å–
- `compression_model` (str) - å‹ç¼©æ—¶ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤"deepseek_chat"
- `save_conversation` (bool) - æ˜¯å¦ä¿å­˜äº¤äº’è®°å½•åˆ°è®°å¿†ï¼Œé»˜è®¤True

### LLMCaller.call() å‚æ•°
- `messages` (List[Dict]) - æ¶ˆæ¯åˆ—è¡¨ï¼Œå¿…éœ€
- `model_name` (str) - æ¨¡å‹åç§°ï¼Œé»˜è®¤"deepseek_chat"
- `memory` (Optional) - è®°å¿†å¯¹è±¡ï¼Œé»˜è®¤None
- `temperature` (Optional[float]) - æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤None

### è®°å¿†ç®¡ç†ä¸“ç”¨å‚æ•°
- `load_memory_by_range()` - æŒ‰èŒƒå›´åŠ è½½ï¼š`start_msg`, `end_msg`, `use_compression`, `compression_model`
- `compress_memory_chunk()` - å‹ç¼©åˆ†ç‰‡ï¼š`chunk_index`, `model_name`, `compression_prompt`
- `batch_compress_memory()` - æ‰¹é‡å‹ç¼©ï¼š`chunk_indices`, `model_name`
- `get_memory_stats()` - è·å–ç»Ÿè®¡ï¼šä»…éœ€`session_id`

## æ–‡ä»¶ç»“æ„
```
langchain/
â”œâ”€â”€ main.py                # ä¸»ç¨‹åºï¼ˆé›†æˆæ™ºèƒ½è®°å¿†ç®¡ç†ï¼‰
â”œâ”€â”€ main_backup.py         # åŸç‰ˆæœ¬å¤‡ä»½
â”œâ”€â”€ enhanced_memory_example.py  # è®°å¿†ç®¡ç†åŠŸèƒ½ç¤ºä¾‹
â”œâ”€â”€ data/                  # æ•°æ®å­˜å‚¨
â”‚   â”œâ”€â”€ chapter_XXX_state.json  # ç« èŠ‚çŠ¶æ€
â”‚   â””â”€â”€ world_bible_XX.json     # ä¸–ç•Œè®¾å®š
â”œâ”€â”€ memory/               # æ™ºèƒ½è®°å¿†ç®¡ç†
â”‚   â”œâ”€â”€ chunks/           # åˆ†ç‰‡å­˜å‚¨
â”‚   â”‚   â”œâ”€â”€ session_chunk_001.json
â”‚   â”‚   â””â”€â”€ session_chunk_002.json
â”‚   â”œâ”€â”€ summaries/        # å‹ç¼©æ‘˜è¦
â”‚   â”‚   â””â”€â”€ session_summary_001.json
â”‚   â””â”€â”€ session_index.json  # ä¼šè¯ç´¢å¼•
â”œâ”€â”€ xiaoshuo/             # ç”Ÿæˆçš„å°è¯´
â”œâ”€â”€ versions/             # ç‰ˆæœ¬ç®¡ç†
â”œâ”€â”€ modules/              # æ—§æ¨¡å—ï¼ˆå¯é€‰æ‹©ä¿ç•™ï¼‰
â””â”€â”€ prompts/              # æç¤ºè¯æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„å¤§æ¨¡å‹
åœ¨ `LLMConfigManager.get_config()` ä¸­æ·»åŠ é…ç½®ï¼š
```python
"new_model": {
    "provider": "custom",
    "model": "model-name",
    "api_key": os.getenv("CUSTOM_API_KEY"),
    "base_url": "https://api.custom.com",
    "temperature": 0.7
}
```

åœ¨ `LLMCaller.call()` ä¸­æ·»åŠ å¯¹åº”çš„providerå¤„ç†é€»è¾‘ã€‚

### è‡ªå®šä¹‰ä¸šåŠ¡é€»è¾‘
ç»§æ‰¿æˆ–ç»„åˆ `NovelGenerator` ç±»ï¼š
```python
class CustomGenerator(NovelGenerator):
    def custom_generate(self, custom_params):
        # è‡ªå®šä¹‰ç”Ÿæˆé€»è¾‘
        return self.generate_chapter(
            chapter_plan=custom_params,
            model_name="custom_model",
            system_prompt="è‡ªå®šä¹‰æç¤ºè¯"
        )
```

## ä¼˜åŠ¿ç‰¹ç‚¹

1. **é›¶ç¡¬ç¼–ç ** - æ‰€æœ‰æç¤ºè¯å’Œå‚æ•°é€šè¿‡æ¥å£ä¼ å…¥
2. **æ’ä»¶æ¶æ„** - å¤§æ¨¡å‹é…ç½®å’Œè°ƒç”¨å®Œå…¨è§£è€¦
3. **å‚æ•°åŒ–æ§åˆ¶** - æ¯ä¸ªåŠŸèƒ½éƒ½å¯é€šè¿‡å‚æ•°ç²¾ç¡®æ§åˆ¶
4. **é«˜åº¦å¤ç”¨** - å…¨å±€LLMè°ƒç”¨å™¨å¯è¢«æ‰€æœ‰ä¸šåŠ¡ç»„ä»¶ä½¿ç”¨
5. **æ˜“äºæ‰©å±•** - æ·»åŠ æ–°æ¨¡å‹æˆ–åŠŸèƒ½åªéœ€ä¿®æ”¹é…ç½®
6. **æ™ºèƒ½è®°å¿†ç®¡ç†** - åˆ†ç‰‡å­˜å‚¨+å‹ç¼©ï¼Œæ”¯æŒå¤§è§„æ¨¡å¯¹è¯å†å²
7. **ç»Ÿä¸€æ¥å£** - å•ä¸€è®°å¿†ç®¡ç†æ¥å£ï¼ŒåŠŸèƒ½å¼ºå¤§è€Œç®€æ´
8. **ç®€æ´æ˜äº†** - æ ¸å¿ƒä»£ç ä¿æŒæ¸…æ™°ï¼ŒåŠŸèƒ½æ¨¡å—åŒ–

## å†å²è®°å½•è¯»å–æœºåˆ¶è¯¦è§£

### å‹ç¼©æ§åˆ¶å‚æ•°çš„å…·ä½“è¡Œä¸º
å½“ä½¿ç”¨ `generate_chapter()` æˆ– `chat()` æ–¹æ³•æ—¶ï¼š

**use_compression=False (é»˜è®¤)**ï¼š
- è¯»å–è·¯å¾„ï¼š`memory/chunks/{session_id}_chunk_xxx.json`
- å†…å®¹ï¼šåŸå§‹å®Œæ•´çš„å¯¹è¯æ¶ˆæ¯
- é€‚ç”¨åœºæ™¯ï¼šçŸ­æœŸå¯¹è¯ï¼Œéœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡

**use_compression=True**ï¼š
- è¯»å–è·¯å¾„ï¼š`memory/summaries/{session_id}_summary_xxx.json`
- å†…å®¹ï¼šLLMå‹ç¼©åçš„æ‘˜è¦æ–‡æœ¬
- é€‚ç”¨åœºæ™¯ï¼šé•¿æœŸé¡¹ç›®ï¼ŒèŠ‚çœtokenæ¶ˆè€—

### æ–‡ä»¶è·¯å¾„å¯¹åº”å…³ç³»
```
memory/
â”œâ”€â”€ chunks/                           # åŸå§‹æ¶ˆæ¯å­˜å‚¨
â”‚   â”œâ”€â”€ novel_project_1_chunk_001.json    # ç¬¬1-100æ¡æ¶ˆæ¯
â”‚   â””â”€â”€ novel_project_1_chunk_002.json    # ç¬¬101-200æ¡æ¶ˆæ¯
â”œâ”€â”€ summaries/                        # å‹ç¼©æ‘˜è¦å­˜å‚¨  
â”‚   â””â”€â”€ novel_project_1_summary_001.json  # ç¬¬1ç‰‡çš„å‹ç¼©æ‘˜è¦
â””â”€â”€ novel_project_1_index.json       # ç´¢å¼•æ–‡ä»¶
```

### è°ƒç”¨ç¤ºä¾‹å¯¹æ¯”
```python
# è¯»å–åŸå§‹æ¶ˆæ¯ (å®Œæ•´ä¸Šä¸‹æ–‡)
response = generator.chat(
    user_input="ç»§ç»­å†™ä½œ",
    use_compression=False,  # ä»chunks/ç›®å½•è¯»å–
    recent_count=20
)

# è¯»å–å‹ç¼©æ‘˜è¦ (èŠ‚çœtoken)
response = generator.chat(
    user_input="ç»§ç»­å†™ä½œ", 
    use_compression=True,   # ä»summaries/ç›®å½•è¯»å–
    recent_count=20
)
```

## æ™ºèƒ½è®°å¿†ç®¡ç†ç³»ç»Ÿ

### æ ¸å¿ƒç»„ä»¶
- **MemoryChunkManager** - åˆ†ç‰‡å­˜å‚¨ç®¡ç†å™¨ï¼Œå¤„ç†æ¶ˆæ¯åˆ†ç‰‡å’Œç´¢å¼•
- **MemoryCompressor** - ç‹¬ç«‹å‹ç¼©æ¨¡å—ï¼Œä½¿ç”¨LLMå‹ç¼©å†å²è®°å½•
- **MemoryIndexManager** - ç´¢å¼•ç®¡ç†å™¨ï¼Œç»´æŠ¤åˆ†ç‰‡ä¿¡æ¯å’Œå…ƒæ•°æ®
- **MemoryManager** - æ™ºèƒ½è®°å¿†ç®¡ç†å™¨ï¼Œæ•´åˆä¸Šè¿°åŠŸèƒ½

### å·¥ä½œåŸç†
1. **åˆ†ç‰‡å­˜å‚¨** - æŒ‰è®¾å®šå¤§å°å°†æ¶ˆæ¯åˆ†ç‰‡å­˜å‚¨ï¼ˆé»˜è®¤100æ¡/ç‰‡ï¼‰
2. **ç´¢å¼•ç®¡ç†** - ç»´æŠ¤åˆ†ç‰‡ç´¢å¼•ï¼Œæ”¯æŒå¿«é€Ÿå®šä½å’ŒèŒƒå›´æŸ¥è¯¢
3. **æ™ºèƒ½å‹ç¼©** - å¯é€‰æ‹©æ€§å‹ç¼©å†å²åˆ†ç‰‡ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´
4. **çµæ´»åŠ è½½** - æ”¯æŒæŒ‰èŒƒå›´ã€æœ€è¿‘Næ¡ç­‰å¤šç§åŠ è½½æ–¹å¼

### æ–‡ä»¶ç»“æ„
```
memory/
â”œâ”€â”€ chunks/                    # åˆ†ç‰‡å­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ {session_id}_chunk_001.json
â”‚   â””â”€â”€ {session_id}_chunk_002.json
â”œâ”€â”€ summaries/                 # å‹ç¼©æ‘˜è¦ç›®å½•
â”‚   â””â”€â”€ {session_id}_summary_001.json
â””â”€â”€ {session_id}_index.json   # ä¼šè¯ç´¢å¼•æ–‡ä»¶
```

### ä½¿ç”¨åœºæ™¯
- **çŸ­æœŸå¯¹è¯** - é»˜è®¤æ¨¡å¼ï¼Œè‡ªåŠ¨ç®¡ç†è®°å¿†
- **é•¿æœŸé¡¹ç›®** - å¯ç”¨å‹ç¼©ï¼Œæ”¯æŒæ•°åƒæ¡æ¶ˆæ¯
- **æ‰¹é‡å¤„ç†** - æ”¯æŒæ‰¹é‡å‹ç¼©å’ŒèŒƒå›´æŸ¥è¯¢
- **çµæ´»é…ç½®** - å¯è°ƒèŠ‚åˆ†ç‰‡å¤§å°å’Œå‹ç¼©ç­–ç•¥

## è¿ç§»æŒ‡å—

ä»æ—§ç‰ˆæœ¬è¿ç§»ï¼š
1. å¤‡ä»½ç°æœ‰æ•°æ®æ–‡ä»¶ï¼ˆdata/, memory/ç­‰ï¼‰
2. ä½¿ç”¨æ–°çš„ `NovelGenerator` æ›¿ä»£ `NovelGenerationTask`
3. å°†ç¡¬ç¼–ç çš„æç¤ºè¯æ”¹ä¸ºå‚æ•°ä¼ å…¥
4. ä½¿ç”¨ `LLMCaller.call()` æ›¿ä»£ç›´æ¥çš„LLMè°ƒç”¨
5. æ‰€æœ‰è®°å¿†åŠŸèƒ½å·²è‡ªåŠ¨å‡çº§ä¸ºæ™ºèƒ½ç®¡ç†

æ–°æ¶æ„ä¿æŒäº†æ•°æ®æ ¼å¼å…¼å®¹æ€§ï¼Œç°æœ‰çš„çŠ¶æ€æ–‡ä»¶å’Œè®°å¿†æ–‡ä»¶å¯ç›´æ¥ä½¿ç”¨ã€‚

## âš ï¸ é‡è¦è­¦å‘Š

### æ¨¡å‹é…ç½®ä¿æŠ¤
ä»¥ä¸‹ä¸‰ä¸ªæ¨¡å‹é…ç½®ä¸ºç”¨æˆ·å›ºå®šè®¾ç½®ï¼Œ**ä¸¥ç¦åœ¨ä»»ä½•ä»£ç ä¿®æ”¹ä¸­å˜æ›´**ï¼š

1. **deepseek_chat** (é»˜è®¤æ¨¡å‹)
   - API Key: `DEEPSEEK_API_KEY`
   - Base URL: `https://api.deepseek.com/v1`
   - Model: `deepseek-chat`

2. **deepseek_reasoner**
   - API Key: `DEEPSEEK_API_KEY` 
   - Base URL: `https://api.deepseek.com/v1`
   - Model: `deepseek-reasoner`

3. **dsf5**
   - API Key: `DSF5_API_KEY`
   - Base URL: `https://api.sikong.shop/v1`
   - Model: `[ç¨³å®š]gemini-2.5-pro-preview-06-05-c`

è¿™äº›é…ç½®å·²åœ¨ä»£ç ä¸­æ ‡è®°ä¿æŠ¤ï¼Œä»»ä½•ä¿®æ”¹éƒ½ä¼šå¯¼è‡´ç”¨æˆ·è®¾ç½®ä¸¢å¤±ã€‚